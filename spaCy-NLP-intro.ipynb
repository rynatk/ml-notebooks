{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import SpaCy\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Process sentences 'Hello, world. Natural Language Processing in 10 lines of code.' using spaCy\n",
    "doc = nlp(u'Hello, world. Natural Language Processing in 10 lines of code.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of\n",
      "Hello, world.\n",
      "Natural Language Processing in 10 lines of code.\n"
     ]
    }
   ],
   "source": [
    "# Get the first token of the processed document\n",
    "token = doc[10]\n",
    "print(token)\n",
    "\n",
    "# Print sentences (one sentence per line)\n",
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      ",\n",
      "world\n",
      ".\n",
      "Natural\n",
      "Language\n",
      "Processing\n",
      "in\n",
      "10\n",
      "lines\n",
      "of\n",
      "code\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# playing around\n",
    "for i in range(len(doc)):\n",
    "    print(doc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      ",\n",
      "world\n",
      ".\n",
      "Natural\n",
      "Language\n",
      "Processing\n",
      "in\n",
      "10\n",
      "lines\n",
      "of\n",
      "code\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for item in doc:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello - INTJ\n",
      ", - PUNCT\n",
      "world - NOUN\n",
      ". - PUNCT\n",
      "Natural - PROPN\n",
      "Language - PROPN\n",
      "Processing - PROPN\n",
      "in - ADP\n",
      "10 - NUM\n",
      "lines - NOUN\n",
      "of - ADP\n",
      "code - NOUN\n",
      ". - PUNCT\n"
     ]
    }
   ],
   "source": [
    "# For each token, print corresponding part of speech tag\n",
    "for token in doc:\n",
    "    print('{} - {}'.format(token, token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello --> []\n",
      ", --> [,, Hello]\n",
      "world --> [world, Hello]\n",
      ". --> [., Hello]\n",
      "Natural --> [Natural, Language, Language, Processing]\n",
      "Language --> [Language, Processing]\n",
      "Processing --> []\n",
      "in --> [in, Processing]\n",
      "10 --> [10, lines, lines, in, in, Processing]\n",
      "lines --> [lines, in, in, Processing]\n",
      "of --> [of, lines, lines, in, in, Processing]\n",
      "code --> [code, of, of, lines, lines, in, in, Processing]\n",
      ". --> [., Processing]\n",
      "\n",
      ",-punct-> Hello-ROOT\n",
      "world-npadvmod-> Hello-ROOT\n",
      ".-punct-> Hello-ROOT\n",
      "Natural-compound-> Language-compound-> Language-compound-> Processing-ROOT\n",
      "Language-compound-> Processing-ROOT\n",
      "\n",
      "in-prep-> Processing-ROOT\n",
      "10-nummod-> lines-pobj-> lines-pobj-> in-prep-> in-prep-> Processing-ROOT\n",
      "lines-pobj-> in-prep-> in-prep-> Processing-ROOT\n",
      "of-prep-> lines-pobj-> lines-pobj-> in-prep-> in-prep-> Processing-ROOT\n",
      "code-pobj-> of-prep-> of-prep-> lines-pobj-> lines-pobj-> in-prep-> in-prep-> Processing-ROOT\n",
      ".-punct-> Processing-ROOT\n"
     ]
    }
   ],
   "source": [
    "# Write a function that walks up the syntactic tree of the given token and collects all tokens to the root token( including the root token).\n",
    "def tokens_to_root(token):\n",
    "    \"\"\"\n",
    "    Walk up the syntactic tree, collecting tokens to the root of the given `token`.\n",
    "    :param token: SpaCy token\n",
    "    :return: list of SpaCy tokens\n",
    "    \"\"\"\n",
    "    tokens_to_r = []\n",
    "    while token.head is not token:\n",
    "        tokens_to_r.append(token)\n",
    "        token = token.head\n",
    "        tokens_to_r.append(token)\n",
    "        \n",
    "    return tokens_to_r\n",
    "\n",
    "# For every token in dopcument, print its tokens to the root\n",
    "for token in doc:\n",
    "    print('{} --> {}'.format(token, tokens_to_root(token)))\n",
    "    \n",
    "# Print dependency labels of the tokens\n",
    "for token in doc:\n",
    "    print('-> '.join(['{}-{}'.format(dependent_token, dependent_token.dep_) for dependent_token in tokens_to_root(token)]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris - GPE\n",
      "Jack - PERSON\n"
     ]
    }
   ],
   "source": [
    "# Print all named entities with named entity types\n",
    "\n",
    "doc_2 = nlp(u\"I went to Paris where I met my old friend Jack from uni.\")\n",
    "for ent in doc_2.ents:\n",
    "    print('{} - {}'.format(ent, ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I, Paris, I, my old friend, uni]\n"
     ]
    }
   ],
   "source": [
    "# Print noun chunks for doc_2\n",
    "print([chunk for chunk in doc_2.noun_chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For every token in doc_2 print lo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
